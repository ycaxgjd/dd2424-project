{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quantized distillation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO5uerZdpxWk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code mainly refers to https://github.com/antspy/quantized_distillation\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Functional\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.nn.init import xavier_uniform, calculate_gain\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet34\n",
        "from torchvision.models import resnet18\n",
        "from google.colab import drive\n",
        "import functools\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from heapq import heappush, heappop, heapify\n",
        "\n",
        "\n",
        "'''\n",
        "Implements the model and training techniques detailed in the paper:\n",
        "\"Do deep convolutional neural network really need to be deep and convolutional?\"\n",
        "'''\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "  \n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, stride=stride, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, stride=1, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inplanes != planes * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    cfg = {\n",
        "        18: [2, 2, 2, 2],\n",
        "        34: [3, 4, 6, 3],\n",
        "    }\n",
        "    cfg_pm = {\n",
        "        18: resnet18(pretrained=True),\n",
        "        34: resnet34(pretrained=True),\n",
        "    }\n",
        "\n",
        "    def __init__(self, key, pretrained=False, k=1):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.grams = None\n",
        "        if pretrained:\n",
        "            self._pm = self.cfg_pm[key]\n",
        "            self._pm.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            self._pm.fc = nn.Linear(self._pm.fc.in_features, 10)\n",
        "            for _para in list(self._pm.parameters()):\n",
        "                _para.requires_grad = False\n",
        "        else:\n",
        "            self.inplanes = 64*k\n",
        "            self.conv1 = nn.Conv2d(3, 64*k, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64*k)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            self.layer1 = self._make_layer(64*k, self.cfg[key][0], stride=1)\n",
        "            self.layer2 = self._make_layer(128*k, self.cfg[key][1], stride=2)\n",
        "            self.layer3 = self._make_layer(256*k, self.cfg[key][2], stride=2)\n",
        "            self.layer4 = self._make_layer(512*k, self.cfg[key][3], stride=2)\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.fc = nn.Linear(512 * k * BasicBlock.expansion, 10)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride=1):\n",
        "        layers = list()\n",
        "        layers.append(BasicBlock(self.inplanes, planes, stride))\n",
        "        self.inplanes = planes * BasicBlock.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.grams = list()\n",
        "        if self.pretrained:\n",
        "            x = self._pm.conv1(x)\n",
        "            x = self._pm.bn1(x)\n",
        "            x = self._pm.relu(x)\n",
        "            # x = self._pm.maxpool(x)\n",
        "\n",
        "            x = self._pm.layer1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer4(x)\n",
        "            self.grams.append(x)\n",
        "\n",
        "            x = self._pm.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self._pm.fc(x)\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            #             x = self.maxpool(x)\n",
        "\n",
        "            x = self.layer1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer4(x)\n",
        "            self.grams.append(x)\n",
        "\n",
        "            x = self.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ConvolForwardNet(nn.Module):\n",
        "    ''' Teacher model as described in the paper :\n",
        "    \"Do deep convolutional neural network really need to be deep and convolutional?\"'''\n",
        "\n",
        "    def __init__(self, width, height, spec_conv_layers, spec_max_pooling, spec_linear, spec_dropout_rates):\n",
        "\n",
        "        '''\n",
        "        The structure of the network is: a number of convolutional layers, intermittend max-pooling and dropout layers,\n",
        "        and a number of linear layers. The max-pooling layers are inserted in the positions specified, as do the dropout\n",
        "        layers.\n",
        "\n",
        "        :param spec_conv_layers: list of tuples with (numFilters, width, height) (one tuple for each layer);\n",
        "        :param spec_max_pooling: list of tuples with (posToInsert, width, height) of max-pooling layers\n",
        "        :param spec_dropout_rates list of tuples with (posToInsert, rate of dropout) (applied after max-pooling)\n",
        "        :param spec_linear: list with numNeurons for each layer (i.e. [100, 200, 300] creates 3 layers)\n",
        "        '''\n",
        "\n",
        "        super(ConvolForwardNet, self).__init__()\n",
        "\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.conv_layers = []\n",
        "        self.max_pooling_layers = []\n",
        "        self.dropout_layers = []\n",
        "        self.linear_layers = []\n",
        "        self.max_pooling_positions = []\n",
        "        self.dropout_positions = []\n",
        "        self.batchNormalizationLayers = []\n",
        "\n",
        "        # creating the convolutional layers\n",
        "        oldNumChannels = 3\n",
        "        for idx in range(len(spec_conv_layers)):\n",
        "            currSpecLayer = spec_conv_layers[idx]\n",
        "            numFilters = currSpecLayer[0]\n",
        "            kernel_size = (currSpecLayer[1], currSpecLayer[2])\n",
        "            # The padding needs to be such that width and height of the image are unchanges after each conv layer\n",
        "            padding = ((kernel_size[0] - 1) // 2, (kernel_size[1] - 1) // 2)\n",
        "            newConvLayer = nn.Conv2d(in_channels=oldNumChannels, out_channels=numFilters,\n",
        "                                     kernel_size=kernel_size, padding=padding)\n",
        "            xavier_uniform(newConvLayer.weight, calculate_gain('conv2d'))  # glorot weight initialization\n",
        "            self.conv_layers.append(newConvLayer)\n",
        "            self.batchNormalizationLayers.append(nn.BatchNorm2d(numFilters, affine=True))\n",
        "            oldNumChannels = numFilters\n",
        "\n",
        "        # creating the max pooling layers\n",
        "        for idx in range(len(spec_max_pooling)):\n",
        "            currSpecLayer = spec_max_pooling[idx]\n",
        "            kernel_size = (currSpecLayer[1], currSpecLayer[2])\n",
        "            self.max_pooling_layers.append(nn.MaxPool2d(kernel_size))\n",
        "            self.max_pooling_positions.append(currSpecLayer[0])\n",
        "\n",
        "        # creating the dropout layers\n",
        "        for idx in range(len(spec_dropout_rates)):\n",
        "            currSpecLayer = spec_dropout_rates[idx]\n",
        "            rate = currSpecLayer[1]\n",
        "            currPosition = currSpecLayer[0]\n",
        "            if currPosition < len(self.conv_layers):\n",
        "                # we use dropout2d only for the conv_layers, otherwise we use the usual dropout\n",
        "                self.dropout_layers.append(nn.Dropout2d(rate))\n",
        "            else:\n",
        "                self.dropout_layers.append(nn.Dropout(rate))\n",
        "            self.dropout_positions.append(currPosition)\n",
        "\n",
        "        # creating the linear layers\n",
        "        oldInputFeatures = oldNumChannels * width * height // 2 ** (2 * len(self.max_pooling_layers))\n",
        "        for idx in range(len(spec_linear)):\n",
        "            currNumFeatures = spec_linear[idx]\n",
        "            newLinearLayer = nn.Linear(in_features=oldInputFeatures, out_features=currNumFeatures)\n",
        "            xavier_uniform(newLinearLayer.weight, calculate_gain('linear'))  # glorot weight initialization\n",
        "            self.linear_layers.append(newLinearLayer)\n",
        "            self.batchNormalizationLayers.append(nn.BatchNorm1d(currNumFeatures, affine=True))\n",
        "            oldInputFeatures = currNumFeatures\n",
        "\n",
        "        # final output layer\n",
        "        self.out_layer = nn.Linear(in_features=oldInputFeatures, out_features=10)\n",
        "        xavier_uniform(self.out_layer.weight, calculate_gain('linear'))\n",
        "\n",
        "        self.conv_layers = nn.ModuleList(self.conv_layers)\n",
        "        self.max_pooling_layers = nn.ModuleList(self.max_pooling_layers)\n",
        "        self.dropout_layers = nn.ModuleList(self.dropout_layers)\n",
        "        self.linear_layers = nn.ModuleList(self.linear_layers)\n",
        "        self.batchNormalizationLayers = nn.ModuleList(self.batchNormalizationLayers)\n",
        "        self.num_conv_layers = len(self.conv_layers)\n",
        "        self.total_num_layers = self.num_conv_layers + len(self.linear_layers)\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        for idx in range(self.total_num_layers):\n",
        "            if idx < self.num_conv_layers:\n",
        "                input = Functional.relu(self.conv_layers[idx](input))\n",
        "            else:\n",
        "                if idx == self.num_conv_layers:\n",
        "                    # if it is the first layer after the convolutional layers, make it as a vector\n",
        "                    input = input.view(input.size()[0], -1)\n",
        "                input = Functional.relu(self.linear_layers[idx - self.num_conv_layers](input))\n",
        "\n",
        "            input = self.batchNormalizationLayers[idx](input)\n",
        "\n",
        "            try:\n",
        "                posMaxLayer = self.max_pooling_positions.index(idx)\n",
        "                input = self.max_pooling_layers[posMaxLayer](input)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "            try:\n",
        "                posDropoutLayer = self.dropout_positions.index(idx)\n",
        "                input = self.dropout_layers[posDropoutLayer](input)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        input = Functional.relu(self.out_layer(input))\n",
        "\n",
        "        # No need to take softmax if the loss function is cross entropy\n",
        "        return input\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, test_loader, initial_learning_rate=0.001, initial_momentum=0.9,\n",
        "                weight_decayL2=0.00022, epochs_to_train=300, use_distillation_loss=False,\n",
        "                teacher_model=None, quantizeWeights=False, numBits=8, bucket_size=None, backprop_quantization_style='none', estimate_quant_grad_every=1):\n",
        "    # backprop_quantization_style determines how to modify the gradients to take into account the\n",
        "    # quantization function. Specifically, one can use 'none', where gradients are not modified,\n",
        "    # 'truncated', where gradient values outside -1 and 1 are truncated to 0 (as per the paper\n",
        "    # specified in the comments) and 'complicated', which is the temp name for my idea which is slow and complicated\n",
        "    # to compute\n",
        "\n",
        "    if teacher_model is not None:\n",
        "        teacher_model.eval()\n",
        "\n",
        "    lr_scheduler = LearningRateScheduler(initial_learning_rate)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=initial_learning_rate, nesterov=True, momentum=initial_momentum,\n",
        "                          weight_decay=weight_decayL2)\n",
        "    startTime = time.time()\n",
        "\n",
        "    pred_accuracy_epochs = []\n",
        "    percentages_asked_teacher = []\n",
        "    losses_epochs = []\n",
        "    last_loss_saved = float('inf')\n",
        "    step_since_last_grad_quant_estimation = 1\n",
        "\n",
        "    if quantizeWeights:\n",
        "        # uniformLinearScaling\n",
        "        s = 2 ** numBits\n",
        "\n",
        "        quantizeFunctions = lambda x: uniformQuantization(x, s, bucket_size=bucket_size)\n",
        "\n",
        "        def quantize_weights_model(model):\n",
        "            for idx, p in enumerate(model.parameters()):\n",
        "                p.data,_ = quantizeFunctions(p.data)\n",
        "                \n",
        "        def backward_quant_weights_model(model):\n",
        "            if backprop_quantization_style == 'none':\n",
        "                return\n",
        "\n",
        "    for epoch in range(epochs_to_train):\n",
        "        model.train()\n",
        "        print_loss_total = 0\n",
        "        count_asked_teacher = 0\n",
        "        count_asked_total = 0\n",
        "        for idx_minibatch, data in enumerate(train_loader, start=1):\n",
        "\n",
        "            if quantizeWeights:\n",
        "                if step_since_last_grad_quant_estimation >= 1:\n",
        "                    # we save them because we only want to quantize weights to compute gradients,\n",
        "                    # but keep using non-quantized weights during the algorithm\n",
        "                    model_state_dict = model.state_dict()\n",
        "                    quantize_weights_model(model)\n",
        "\n",
        "            model.zero_grad()\n",
        "            print_loss, curr_c_teach, curr_c_total = forward_and_backward(model, data,\n",
        "                                                                          use_distillation_loss=use_distillation_loss,\n",
        "                                                                          teacher_model=teacher_model)\n",
        "            count_asked_teacher += curr_c_teach\n",
        "            count_asked_total += curr_c_total\n",
        "\n",
        "            # load the non-quantize weights and use them for the update. The quantized\n",
        "            # weights are used only to get the quantized gradient\n",
        "            if quantizeWeights:\n",
        "                if step_since_last_grad_quant_estimation >= 1:\n",
        "                    model.load_state_dict(model_state_dict)\n",
        "                    del model_state_dict  # free memory                    \n",
        "                if step_since_last_grad_quant_estimation >= estimate_quant_grad_every:\n",
        "                    backward_quant_weights_model(model)\n",
        "                    \n",
        "            optimizer.step()\n",
        "\n",
        "            if step_since_last_grad_quant_estimation >= 1:\n",
        "                step_since_last_grad_quant_estimation = 0\n",
        "\n",
        "            step_since_last_grad_quant_estimation += 1\n",
        "\n",
        "            # print statistics\n",
        "            print_loss_total += print_loss\n",
        "            if idx_minibatch % 500 == 0:\n",
        "                last_loss_saved = print_loss_total / 500\n",
        "\n",
        "                str_to_print = 'Time Elapsed: {}, [Start Epoch: {}, Epoch: {}, Minibatch: {}], loss: {:3f}'.format(\n",
        "                    datetime.fromtimestamp(time.time() - startTime).strftime('%H:%M:%S'), 1, epoch + 1, idx_minibatch,\n",
        "                    last_loss_saved)\n",
        "                if pred_accuracy_epochs:\n",
        "                    str_to_print += ' Last prediction accuracy: {:2f}%'.format(pred_accuracy_epochs[-1] * 100)\n",
        "                print(str_to_print)\n",
        "                print_loss_total = 0\n",
        "\n",
        "        curr_percentages_asked_teacher = count_asked_teacher / count_asked_total if count_asked_total != 0 else 0\n",
        "        percentages_asked_teacher.append(curr_percentages_asked_teacher)\n",
        "        losses_epochs.append(last_loss_saved)\n",
        "        curr_pred_accuracy = evaluateModel(model, test_loader)\n",
        "        pred_accuracy_epochs.append(curr_pred_accuracy)\n",
        "        print(' === Epoch: {} - prediction accuracy {:2f}% === '.format(epoch + 1, curr_pred_accuracy * 100))\n",
        "\n",
        "        # updating the learning rate\n",
        "        new_learning_rate, stop_training = lr_scheduler.update_learning_rate(1 - curr_pred_accuracy)\n",
        "        if stop_training is True:\n",
        "            break\n",
        "        for p in optimizer.param_groups:\n",
        "            p['lr'] = new_learning_rate\n",
        "\n",
        "    if quantizeWeights:\n",
        "        quantize_weights_model(model)\n",
        "\n",
        "    print(f'percentages_asked_teacher {percentages_asked_teacher}')\n",
        "    print(f'predictionAccuracy {pred_accuracy_epochs}')\n",
        "#     print(f'lossSaved {losses_epochs}')\n",
        "#     model_param_save_name = f'resnet18_quantized_param1.pt'\n",
        "#     path = F\"/content/gdrive/My Drive/{model_param_save_name}\" \n",
        "#     torch.save(model.state_dict(), path)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ScalingFunction(object):\n",
        "    '''\n",
        "    This class is there to hold two functions: the scaling function for a tensor, and its inverse.\n",
        "    They are budled together in a class because to be able to invert the scaling, we need to remember\n",
        "    several parameters, and it is a little uncomfortable to do it manually. The class of course remembers\n",
        "    correctly.\n",
        "    '''\n",
        "\n",
        "    # TODO: Make static version of scale and inv_scale that take as arguments all that is necessary,\n",
        "    # and then the class can just be a small wrapper about calling scale, saving the arguments,\n",
        "    # and calling inv. So we would have both ways to call the scaling function, directly and through\n",
        "    # the class.\n",
        "\n",
        "    def __init__(self, bucket_size):\n",
        "        self.bucket_size = bucket_size\n",
        "        self.tol_diff_zero = 1e-10\n",
        "\n",
        "        # Things we need to invert the tensor. Set to None, will be populated by scale\n",
        "        self.mean_tensor = None\n",
        "        self.original_tensor_size = None\n",
        "        self.original_tensor_length = None\n",
        "        self.expected_tensor_size = None\n",
        "\n",
        "        self.alpha = None\n",
        "        self.beta = None\n",
        "\n",
        "    def scale_down(self, tensor):\n",
        "        '''\n",
        "        Scales the tensor using one of the methods. Note that if bucket_size is not None,\n",
        "        the shape of the tensor will be changed. This change will be inverted by inv_scale\n",
        "        '''\n",
        "        self.mean_tensor = 0\n",
        "\n",
        "        self.original_tensor_size = tensor.size()\n",
        "        self.original_tensor_length = tensor.numel()\n",
        "        tensor = create_bucket_tensor(tensor, self.bucket_size)\n",
        "        if self.bucket_size is None:\n",
        "            tensor = tensor.view(-1)\n",
        "        self.expected_tensor_size = tensor.size()\n",
        "\n",
        "        # if tensor is bucketed, it has 2 dimension, otherwise it has 1.\n",
        "        if self.bucket_size is None:\n",
        "            min_rows, _ = tensor.min(dim=0, keepdim=True)\n",
        "            max_rows, _ = tensor.max(dim=0, keepdim=True)\n",
        "        else:\n",
        "            min_rows, _ = tensor.min(dim=1, keepdim=True)\n",
        "            max_rows, _ = tensor.max(dim=1, keepdim=True)\n",
        "        alpha = max_rows - min_rows\n",
        "        beta = min_rows\n",
        "        # If alpha is zero for one row, it means the whole row is 0.\n",
        "        # So we set alpha = 1 there, to avoid nan and inf, and result won't change\n",
        "        if self.bucket_size is None:\n",
        "            if alpha[0] < self.tol_diff_zero:\n",
        "                alpha[0] = 1\n",
        "        else:\n",
        "            alpha[alpha < self.tol_diff_zero] = 1\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "        tensor.sub_(self.beta.expand_as(tensor))\n",
        "        tensor.div_(self.alpha.expand_as(tensor))\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def inv_scale_down(self, tensor):\n",
        "\n",
        "        \"inverts the scaling done before. Note that the max_element truncation won't be inverted\"\n",
        "        if tensor.size() != self.expected_tensor_size:\n",
        "            raise ValueError('The tensor passed has not the expected size.')\n",
        "\n",
        "        tensor.mul_(self.alpha.expand_as(tensor))\n",
        "        tensor.add_(self.beta.expand_as(tensor))\n",
        "\n",
        "        tensor.add_(self.mean_tensor)\n",
        "        tensor = tensor.view(-1)[0:self.original_tensor_length]  # remove the filler values\n",
        "        tensor = tensor.view(self.original_tensor_size)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def uniformQuantization(tensor, s, bucket_size=None):\n",
        "    '''\n",
        "    Quantizes using the random uniform quantization algorithm the tensor passed using s levels.\n",
        "    '''\n",
        "\n",
        "    tensor = tensor.clone()\n",
        "    # we always pass True to modify_in_place because we have already cloned it by this point\n",
        "    scaling_function = ScalingFunction(bucket_size)\n",
        "\n",
        "    tensor = scaling_function.scale_down(tensor)\n",
        "\n",
        "    # decrease s by one so as to have exactly s quantization points\n",
        "    s = s - 1\n",
        "\n",
        "    tensor.mul_(s)\n",
        "    tensor.round_()\n",
        "    tensor.div_(s)\n",
        "\n",
        "    tensor = scaling_function.inv_scale_down(tensor)\n",
        "    return tensor, scaling_function\n",
        "\n",
        "\n",
        "def create_bucket_tensor(tensor, bucket_size):\n",
        "    if bucket_size is None:\n",
        "        return tensor\n",
        "\n",
        "    tensor = tensor.view(-1)\n",
        "\n",
        "    total_length = tensor.numel()\n",
        "    multiple, rest = divmod(total_length, bucket_size)\n",
        "    if multiple != 0 and rest != 0:\n",
        "        # if multiple is 0, the num of elements is smaller than bucket size so we operate directly\n",
        "        # on the tensor passed\n",
        "        values_to_add = torch.ones(bucket_size - rest) * tensor[-1]\n",
        "        values_to_add = values_to_add.to(device)\n",
        "        # add the fill_values to make the tensor a multiple of the bucket size.\n",
        "        tensor = torch.cat([tensor, values_to_add])\n",
        "    if multiple == 0:\n",
        "        # in this case the tensor is smaller than the bucket size. For consistency we still return it in the same\n",
        "        # format (i.e. a row) but the number of elements is smaller (and equal to the lenght of the tensor)\n",
        "        tensor = tensor.view(1, total_length)\n",
        "    else:\n",
        "        # this is the bucket tensor. A view of the original tensor suffice\n",
        "        tensor = tensor.view(-1, bucket_size)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def evaluateModel(model, testLoader, k=1):\n",
        "    model.eval()\n",
        "    correctClass = 0\n",
        "    totalNumExamples = 0\n",
        "\n",
        "    for idx_minibatch, data in enumerate(testLoader):\n",
        "\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        _, topk_predictions = outputs.topk(k, dim=1, largest=True, sorted=True)\n",
        "        topk_predictions = topk_predictions.t()\n",
        "        correct = topk_predictions.eq(labels.view(1, -1).expand_as(topk_predictions))\n",
        "        correctClass += correct.view(-1).float().sum(0, keepdim=True).item()\n",
        "        totalNumExamples += len(labels)\n",
        "\n",
        "    return correctClass / totalNumExamples\n",
        "\n",
        "\n",
        "def forward_and_backward(model, batch, use_distillation_loss=False, teacher_model=None):\n",
        "    # TODO: return_more_info is just there for backward compatibility. A big refactoring is due here, and there one should\n",
        "    # remove the return_more_info flag\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    if use_distillation_loss is True and teacher_model is None:\n",
        "        raise ValueError('To compute distillation loss you need to pass the teacher model')\n",
        "\n",
        "    inputs, labels = batch\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    count_asked_teacher = 0\n",
        "\n",
        "    if use_distillation_loss:\n",
        "        # if cutoff_entropy_value_distillation is not None, we use the distillation loss only on the examples\n",
        "        # whose entropy is higher than the cutoff.\n",
        "\n",
        "        mask_distillation_loss = torch.ByteTensor([True] * outputs.size(0))\n",
        "        index_distillation_loss = torch.arange(0, outputs.size(0))[mask_distillation_loss.view(-1, 1).squeeze()].long()\n",
        "        inverse_idx_distill_loss = torch.arange(0, outputs.size(0))[((1 - mask_distillation_loss.view(-1, 1)).squeeze()).long()].long()\n",
        "#         inverse_idx_distill_loss = torch.arange(0, outputs.size(0))[inverse].long()\n",
        "#         print(inverse_idx_distill_loss)\n",
        "        index_distillation_loss = index_distillation_loss.to(device)\n",
        "        inverse_idx_distill_loss = inverse_idx_distill_loss.to(device)\n",
        "\n",
        "        # this criterion is the distillation criterion according to Hinton's paper:\n",
        "        # \"Distilling the Knowledge in a Neural Network\", Hinton et al.\n",
        "\n",
        "        softmaxFunction = nn.Softmax(dim=1).to(device)\n",
        "        logSoftmaxFunction = nn.LogSoftmax(dim=1).to(device)\n",
        "        KLDivLossFunction = nn.KLDivLoss().to(device)\n",
        "\n",
        "        weight_teacher_loss = 0.7\n",
        "        temperature_distillation = 10\n",
        "        if index_distillation_loss.size() != torch.Size():\n",
        "            count_asked_teacher = index_distillation_loss.numel()\n",
        "            # if index_distillation_loss is not empty\n",
        "            volatile_inputs = inputs.data[index_distillation_loss, :].to(device)\n",
        "            outputsTeacher = teacher_model(volatile_inputs).detach()\n",
        "            loss_masked = weight_teacher_loss * temperature_distillation ** 2 * KLDivLossFunction(\n",
        "                logSoftmaxFunction(outputs[index_distillation_loss, :] / temperature_distillation),\n",
        "                softmaxFunction(outputsTeacher / temperature_distillation))\n",
        "            loss_masked += (1 - weight_teacher_loss) * criterion(outputs[index_distillation_loss, :],\n",
        "                                                                 labels[index_distillation_loss])\n",
        "        else:\n",
        "            loss_masked = 0\n",
        "\n",
        "        if inverse_idx_distill_loss.size() != torch.Size():\n",
        "#             if inverse_idx_distill is not empty\n",
        "#             if len(inverse_idx_distill_loss) !=0 :\n",
        "#                 print(\"aaaaa\")\n",
        "#                 loss_normal = criterion(outputs[inverse_idx_distill_loss, :], labels[inverse_idx_distill_loss])\n",
        "#             else:\n",
        "#                 loss_normal = 0\n",
        "             loss_normal = criterion(outputs[inverse_idx_distill_loss, :], labels[inverse_idx_distill_loss])\n",
        "        else:\n",
        "            loss_normal = 0\n",
        "        loss = loss_masked + loss_normal\n",
        "    else:\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    count_total = inputs.size(0)\n",
        "    return loss.item(), count_asked_teacher, count_total\n",
        "\n",
        "\n",
        "class LearningRateScheduler:\n",
        "    def __init__(self, initial_learning_rate):\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.current_learning_rate = initial_learning_rate\n",
        "\n",
        "        self.old_validation_error = float('inf')\n",
        "        self.epochs_since_validation_error_dropped = 0\n",
        "        self.total_number_of_learning_rate_halves = 0\n",
        "        self.epochs_to_wait_for_halving = 0\n",
        "\n",
        "    def update_learning_rate(self, validation_error):\n",
        "        epoch_to_wait_before_reducing_rate = 10\n",
        "        epochs_to_wait_after_halving = 8\n",
        "        epoch_to_wait_before_stopping = 30\n",
        "        total_halves_before_stopping = 11\n",
        "\n",
        "        new_learning_rate = self.current_learning_rate\n",
        "        stop_training = False\n",
        "\n",
        "        # we have a 0.1% error band\n",
        "        if validation_error + 0.001 < self.old_validation_error:\n",
        "            self.old_validation_error = validation_error\n",
        "            self.epochs_since_validation_error_dropped = 0\n",
        "        else:\n",
        "            self.epochs_since_validation_error_dropped += 1\n",
        "\n",
        "        self.epochs_to_wait_for_halving = max(self.epochs_to_wait_for_halving - 1, 0)\n",
        "        if self.epochs_since_validation_error_dropped >= epoch_to_wait_before_reducing_rate and \\\n",
        "                self.epochs_to_wait_for_halving == 0:\n",
        "            # if validation error does not drop for 10 epochs in a row, halve the learning rate\n",
        "            # but don't halve it for at least 8 epochs after halving.\n",
        "            self.epochs_to_wait_for_halving = epochs_to_wait_after_halving\n",
        "            self.total_number_of_learning_rate_halves += 1\n",
        "            new_learning_rate = self.current_learning_rate / 2\n",
        "            self.current_learning_rate = new_learning_rate\n",
        "\n",
        "        if self.epochs_since_validation_error_dropped > epoch_to_wait_before_stopping or \\\n",
        "                self.total_number_of_learning_rate_halves > total_halves_before_stopping:\n",
        "            # stop training if validation rate hasn't dropped in 30 epochs or if learning rates was halved 11 times already\n",
        "            # i.e. it was reduced by 2048 times.\n",
        "            stop_training = True\n",
        "\n",
        "        return new_learning_rate, stop_training\n",
        "\n",
        "      \n",
        "  \n",
        "def huffman_encode(symb2freq):\n",
        "\n",
        "    \"\"\"Huffman encode the given dict mapping symbols to weights\"\"\"\n",
        "    #code taken from https://rosettacode.org/wiki/Huffman_coding#Python\n",
        "\n",
        "    heap = [[wt, [sym, \"\"]] for sym, wt in symb2freq.items()]\n",
        "    heapify(heap)\n",
        "    while len(heap) > 1:\n",
        "        lo = heappop(heap)\n",
        "        hi = heappop(heap)\n",
        "        for pair in lo[1:]:\n",
        "            pair[1] = '0' + pair[1]\n",
        "        for pair in hi[1:]:\n",
        "            pair[1] = '1' + pair[1]\n",
        "        heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
        "    return sorted(heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
        "\n",
        "\n",
        "  \n",
        "def get_huffman_encoding_mean_bit_length(model_param_iter, quantization_functions, type_quantization='uniform',\n",
        "                                         s=None):\n",
        "\n",
        "    '''\n",
        "    'returns the mean size of the bit requires to encode everything using huffman encoding'\n",
        "    :param model_param_iter: the iterator returning model parameters\n",
        "    :param quantization_functions: the quantization function to use. Either a single one or a list with as many functions\n",
        "                                   as there are tensors in the model\n",
        "    :param type_quantization:      Uniform or nonUniform. If nonUniform, the model_param_iter must the the original weights,\n",
        "                                   not the quantized ones! If uniform, it doesn't matter.\n",
        "    :return: the mean bit size of encoding the model tensors using huffman encoding\n",
        "    '''\n",
        "\n",
        "    type_quantization = type_quantization.lower()\n",
        "    if type_quantization not in ('uniform', 'nonuniform'):\n",
        "        raise ValueError('type_quantization not recognized')\n",
        "\n",
        "    if s is None and type_quantization == 'uniform':\n",
        "        raise ValueError('If type of quantization is uniform, you must provide s')\n",
        "\n",
        "    if not isinstance(quantization_functions, list):\n",
        "        quantization_functions = [quantization_functions]\n",
        "\n",
        "    single_quant_fun = len(quantization_functions) == 1\n",
        "    total_length = 0\n",
        "    frequency = defaultdict(int)\n",
        "    tol = 1e-5\n",
        "    for idx, param in enumerate(model_param_iter):\n",
        "        param = param.clone()\n",
        "        if hasattr(param, 'data'):\n",
        "            param = param.data\n",
        "\n",
        "        total_length += param.numel()\n",
        "        if single_quant_fun:\n",
        "            quant_fun = quantization_functions[0]\n",
        "        else:\n",
        "            quant_fun = quantization_functions[idx]\n",
        "\n",
        "        if type_quantization == 'uniform':\n",
        "            quant_points = [x / (s-1) for x in range(s)]\n",
        "            q_tensor, scal = quant_fun(param)\n",
        "            numpy_array = scal.scale_down(q_tensor).view(-1)[0:scal.original_tensor_length].cpu().numpy()\n",
        "            bin_around_points = [x - tol for x in quant_points]\n",
        "            bin_indices = np.digitize(numpy_array, bin_around_points).flatten() - 1\n",
        "\n",
        "\n",
        "        unique, counts = np.unique(bin_indices, return_counts=True)\n",
        "        for val, count in zip(unique, counts):\n",
        "            frequency[val] += count\n",
        "\n",
        "    assert total_length == sum(frequency.values())\n",
        "    frequency = {x: y/total_length for x, y in frequency.items()}\n",
        "    huffman_code = huffman_encode(frequency)\n",
        "    mean_bit_length = sum(frequency[x[0]]*len(x[1]) for x in huffman_code)\n",
        "\n",
        "    return mean_bit_length  \n",
        "  \n",
        "def get_size_quantized_model(model, numBits, quantization_functions, bucket_size=256,\n",
        "                             type_quantization='uniform', quantizeFirstLastLayer=True):\n",
        "\n",
        "#     'Returns size in MB'\n",
        "\n",
        "    if numBits is None:\n",
        "        return sum(p.numel() for p in model.parameters()) * 4 / 1000000\n",
        "\n",
        "\n",
        "    numTensors = sum(1 for _ in model.parameters())\n",
        "    if quantizeFirstLastLayer is True:\n",
        "        def get_quantized_params():\n",
        "            return model.parameters()\n",
        "        def get_unquantized_params():\n",
        "            return iter(())\n",
        "    else:\n",
        "        def get_quantized_params():\n",
        "            return  (p for idx, p in enumerate(model.parameters()) if idx not in (0, numTensors - 1))\n",
        "        def get_unquantized_params():\n",
        "            return (p for idx, p in enumerate(model.parameters()) if idx in (0, numTensors - 1))\n",
        "\n",
        "    count_quantized_parameters = sum(p.numel() for p in get_quantized_params())\n",
        "    count_unquantized_parameters = sum(p.numel() for p in get_unquantized_params())\n",
        "\n",
        "    #Now get the best huffmann bit length for the quantized parameters\n",
        "    actual_bit_huffmman = get_huffman_encoding_mean_bit_length(get_quantized_params(), quantization_functions,\n",
        "                                                                   type_quantization, s=2**numBits)\n",
        "\n",
        "    #Now we can compute the size.\n",
        "    size_mb = 0\n",
        "    size_mb += count_unquantized_parameters*4 #32 bits / 8 = 4 byte per parameter\n",
        "    size_mb += actual_bit_huffmman*count_quantized_parameters/8 #For the quantized parameters we use the mean huffman length\n",
        "    if bucket_size is not None:\n",
        "        size_mb += count_quantized_parameters/bucket_size*8  #for every bucket size, we have to save 2 parameters.\n",
        "                                                             #so we multiply the number of buckets by 2*32/8 = 8\n",
        "    size_mb = size_mb / 1000000 #to bring it in MB\n",
        "    return size_mb\n",
        "  \n",
        "\n",
        "class QD:\n",
        "    def __init__(self):\n",
        "        self.teacher = None\n",
        "        self.model = None\n",
        "        self.epochs = 300\n",
        "        self.train_batch_size = 128  # 25\n",
        "        self.test_batch_size = 128  # 25\n",
        "        self.train_loader = None\n",
        "        self.test_loader = None\n",
        "\n",
        "    def load_data(self):\n",
        "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=self.train_batch_size,\n",
        "                                                        shuffle=True)\n",
        "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=self.test_batch_size, shuffle=False)\n",
        "\n",
        "    def load_model(self):\n",
        "        self.teacher = ResNet(34, False).to(device)\n",
        "        model_param_save_name = f'resnet34_param.pt'\n",
        "        path = F\"/content/gdrive/My Drive/{model_param_save_name}\" \n",
        "        self.teacher.load_state_dict(torch.load(path))\n",
        "        for param in list(self.teacher.parameters()):\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def main(self, quantized=False):\n",
        "        self.load_data()\n",
        "        self.load_model()\n",
        "\n",
        "#         for numBit in [2, 4, 8]:\n",
        "        for numBit in [4]:\n",
        "            model = ResNet(18,False,k=1).to(device)\n",
        "\n",
        "            if quantized:  # distillation with quantized bits\n",
        "                train_model(\n",
        "                    model,\n",
        "                    train_loader=self.train_loader,\n",
        "                    test_loader=self.test_loader,\n",
        "                    **{\n",
        "                        'teacher_model': self.teacher,\n",
        "                        'epochs_to_train' : self.epochs,\n",
        "                        'use_distillation_loss': True,\n",
        "                        'quantizeWeights': True,\n",
        "                        'numBits': numBit,\n",
        "                        'bucket_size': 256,\n",
        "                    }\n",
        "                )\n",
        "                quant_fun = functools.partial(uniformQuantization, s=2**numBit, bucket_size=256)\n",
        "                size_model_MB = get_size_quantized_model(model, numBit, quant_fun, bucket_size=256, quantizeFirstLastLayer=True)\n",
        "#                 print(size_model_MB)\n",
        "                print(F\"The size of the quantized model using Huffman coding is {size_model_MB} MB\")\n",
        "#                 model_param_save_name = 'resnet18_quantized_param.pt'\n",
        "#                 path = F\"/content/gdrive/My Drive/{model_param_save_name}\" \n",
        "#                 torch.save(model.state_dict(), path)\n",
        "            else:  # distillation\n",
        "                train_model(\n",
        "                    model,\n",
        "                    train_loader=self.train_loader,\n",
        "                    test_loader=self.test_loader,\n",
        "                    **{\n",
        "                        'teacher_model': self.teacher,\n",
        "                        'use_distillation_loss': True,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    drive.mount('/content/gdrive')\n",
        "#     QD().main()\n",
        "    QD().main(quantized=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}