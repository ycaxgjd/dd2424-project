{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ycaxgjd/dd2424/blob/master/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHOiJrju2ITE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import pickle\n",
        "import time\n",
        "from datetime import datetime\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "from google.colab import files\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.models import resnet34\n",
        "from torchvision.models import vgg11\n",
        "from torchvision.models import vgg19\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "if device == torch.device('cuda'):\n",
        "    cudnn.benchmark = True\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    cfg = {\n",
        "        11: [[64, 'M'], [128, 'M'], [256, 256, 'M'], [512, 512, 'M'], [512, 512, 'M']],\n",
        "        19: [[64, 64, 'M'], [128, 128, 'M'], [256, 256, 256, 256, 'M'], [512, 512, 512, 512, 'M'],\n",
        "             [512, 512, 512, 512, 'M']],\n",
        "    }\n",
        "    cfg_pm = {\n",
        "        11: vgg11(pretrained=True),\n",
        "        19: vgg19(pretrained=True),\n",
        "    }\n",
        "\n",
        "    def __init__(self, key, pretrained=False):\n",
        "        super(VGG, self).__init__()\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.grams = None\n",
        "\n",
        "        self.in_channels = 3\n",
        "        if pretrained:\n",
        "            self._pm = self.cfg_pm[key]\n",
        "            self._features1 = self._make_layers(self.cfg[key][0])\n",
        "            self._features2 = self._make_layers(self.cfg[key][1])\n",
        "            self._features3 = self._make_layers(self.cfg[key][2])\n",
        "            self._features4 = self._make_layers(self.cfg[key][3])\n",
        "            self._features5 = self._make_layers(self.cfg[key][4])\n",
        "            self._pm.avgpool = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "            self._pm.classifier = nn.Sequential(\n",
        "                nn.Linear(512, 4096),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(4096, 4096),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(4096, 10),\n",
        "            )\n",
        "            for _para in list(self._pm.parameters()):\n",
        "                _para.requires_grad = False\n",
        "        else:\n",
        "            self.features1 = self._make_layers(self.cfg[key][0])\n",
        "            self.features2 = self._make_layers(self.cfg[key][1])\n",
        "            self.features3 = self._make_layers(self.cfg[key][2])\n",
        "            self.features4 = self._make_layers(self.cfg[key][3])\n",
        "            self.features5 = self._make_layers(self.cfg[key][4])\n",
        "            self.avgpool = nn.AvgPool2d(kernel_size=1, stride=1)\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(512, 4096),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(4096, 4096),\n",
        "                nn.ReLU(True),\n",
        "                nn.Dropout(),\n",
        "                nn.Linear(4096, 10),\n",
        "            )\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        for v in cfg:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(self.in_channels, v, kernel_size=3, padding=1)\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "                self.in_channels = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.grams = list()\n",
        "        if self.pretrained:\n",
        "            x = self._features1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._features2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._features3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._features4(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._features5(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self._pm.classifier(x)\n",
        "        else:\n",
        "            x = self.features1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.features2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.features3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.features4(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.features5(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, stride=stride, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, stride=1, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inplanes != planes * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inplanes, planes * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * self.expansion),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    cfg = {\n",
        "        18: [2, 2, 2, 2],\n",
        "        34: [3, 4, 6, 3],\n",
        "    }\n",
        "    cfg_pm = {\n",
        "        18: resnet18(pretrained=True),\n",
        "        34: resnet34(pretrained=True),\n",
        "    }\n",
        "\n",
        "    def __init__(self, key, pretrained=False):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.pretrained = pretrained\n",
        "        self.grams = None\n",
        "        if pretrained:\n",
        "            self._pm = self.cfg_pm[key]\n",
        "            self._pm.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            self._pm.fc = nn.Linear(self._pm.fc.in_features, 10)\n",
        "            for _para in list(self._pm.parameters()):\n",
        "                _para.requires_grad = False\n",
        "        else:\n",
        "            self.inplanes = 64\n",
        "            self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "            self.bn1 = nn.BatchNorm2d(64)\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "            self.layer1 = self._make_layer(64, self.cfg[key][0], stride=1)\n",
        "            self.layer2 = self._make_layer(128, self.cfg[key][1], stride=2)\n",
        "            self.layer3 = self._make_layer(256, self.cfg[key][2], stride=2)\n",
        "            self.layer4 = self._make_layer(512, self.cfg[key][3], stride=2)\n",
        "            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "            self.fc = nn.Linear(512 * BasicBlock.expansion, 10)\n",
        "\n",
        "    def _make_layer(self, planes, blocks, stride=1):\n",
        "        layers = list()\n",
        "        layers.append(BasicBlock(self.inplanes, planes, stride))\n",
        "        self.inplanes = planes * BasicBlock.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.grams = list()\n",
        "        if self.pretrained:\n",
        "            x = self._pm.conv1(x)\n",
        "            x = self._pm.bn1(x)\n",
        "            x = self._pm.relu(x)\n",
        "            # x = self._pm.maxpool(x)\n",
        "\n",
        "            x = self._pm.layer1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self._pm.layer4(x)\n",
        "            self.grams.append(x)\n",
        "\n",
        "            x = self._pm.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self._pm.fc(x)\n",
        "        else:\n",
        "            x = self.conv1(x)\n",
        "            x = self.bn1(x)\n",
        "            x = self.relu(x)\n",
        "            #             x = self.maxpool(x)\n",
        "\n",
        "            x = self.layer1(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer2(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer3(x)\n",
        "            self.grams.append(x)\n",
        "            x = self.layer4(x)\n",
        "            self.grams.append(x)\n",
        "\n",
        "            x = self.avgpool(x)\n",
        "            x = x.view(x.size(0), -1)\n",
        "            x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Teacher(object):\n",
        "    def __init__(self, pretrained=False, teacher_id='resnet34'):\n",
        "        self.pretrained = pretrained\n",
        "        self.model_id = teacher_id\n",
        "        self.model = None\n",
        "        self.epochs = 200\n",
        "        self.train_batch_size = 128\n",
        "        self.test_batch_size = 128\n",
        "        self.train_loader = None\n",
        "        self.test_loader = None\n",
        "        self.criterion = None\n",
        "        self.optimizer = None\n",
        "        self.scheduler = None\n",
        "\n",
        "    def load_data(self):\n",
        "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=self.train_batch_size,\n",
        "                                                        shuffle=True)\n",
        "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=self.test_batch_size, shuffle=False)\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.model_id == 'vgg19':\n",
        "            self.model = VGG(19, self.pretrained).to(device)\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "        elif self.model_id == 'resnet34':\n",
        "            self.model = ResNet(34, self.pretrained).to(device)\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "        # self.model = torch.nn.DataParallel(self.model)\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[75, 150], gamma=0.5)\n",
        "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        last_time = time.time()\n",
        "        for batch_num, (data, target) in enumerate(self.train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            prediction = torch.max(output, 1)  # second param '1' represents the dimension to be reduced\n",
        "            total += target.size(0)\n",
        "            # train_correct incremented by one if predicted right\n",
        "            train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
        "\n",
        "        acc_msg = train_correct / total\n",
        "        loss_msg = train_loss / self.train_batch_size\n",
        "        time_msg = time.time() - last_time\n",
        "        return acc_msg, loss_msg, time_msg\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        test_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        last_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _, (data, target) in enumerate(self.test_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "                test_loss += loss.item()\n",
        "                prediction = torch.max(output, 1)\n",
        "                total += target.size(0)\n",
        "                test_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
        "\n",
        "        acc_msg = test_correct / total\n",
        "        loss_msg = test_loss / self.test_batch_size\n",
        "        time_msg = time.time() - last_time\n",
        "        return acc_msg, loss_msg, time_msg\n",
        "\n",
        "    def run(self):\n",
        "        self.load_data()\n",
        "        self.load_model()\n",
        "\n",
        "        try:\n",
        "            with open(f'sd4{self.model_id}.pkl', 'rb') as file:\n",
        "                best_settings = pickle.load(file)\n",
        "            self.model.load_state_dict(best_settings)\n",
        "            return\n",
        "        except IOError:\n",
        "            pass\n",
        "            # uploaded = files.upload()\n",
        "            # for file_name in uploaded.keys():\n",
        "            #     if file_name == f'sd4{self.model_id}.pkl':\n",
        "            #         with open(file_name, 'rb') as file:\n",
        "            #             best_settings = pickle.load(file)\n",
        "            #         self.model.load_state_dict(best_settings)\n",
        "            #         return\n",
        "\n",
        "        best_acc = 0\n",
        "        best_settings = None\n",
        "        init_time = time.time()\n",
        "        for epoch in range(self.epochs):\n",
        "            self.scheduler.step(epoch)\n",
        "            print('==> Epoch %d/200' % (epoch + 1))\n",
        "\n",
        "            train_acc, train_loss, train_time = self.train()\n",
        "            acc_msg = 'TrainAcc %.3f%%' % (train_acc * 100.0)\n",
        "            loss_msg = 'TrainLoss %.3f' % train_loss\n",
        "            time_msg = datetime.fromtimestamp(train_time).strftime('%H:%M:%S')\n",
        "            print('Train:' + ' | '.join([acc_msg, loss_msg, time_msg]))\n",
        "\n",
        "            test_acc, test_loss, test_time = self.test()\n",
        "            acc_msg = 'TestAcc %.3f%%' % (test_acc * 100.0)\n",
        "            loss_msg = 'TestLoss %.3f' % test_loss\n",
        "            time_msg = datetime.fromtimestamp(test_time).strftime('%H:%M:%S')\n",
        "            print('Test:' + ' | '.join([acc_msg, loss_msg, time_msg]))\n",
        "\n",
        "            if best_acc < test_acc:\n",
        "                best_acc = test_acc\n",
        "                best_settings = copy.deepcopy(self.model.state_dict())\n",
        "        total_time = time.time() - init_time\n",
        "        total_time_msg = datetime.fromtimestamp(total_time).strftime('%H:%M:%S')\n",
        "        print('==> Best TestAcc %.3f%%' % (best_acc * 100))\n",
        "        print('==> Total Time ' + total_time_msg)\n",
        "        self.model.load_state_dict(best_settings)\n",
        "\n",
        "        with open(f'sd4{self.model_id}.pkl', 'wb') as file:\n",
        "            pickle.dump(best_settings, file)\n",
        "        # files.download(f'sd4{self.model_id}.pkl')\n",
        "\n",
        "\n",
        "class Student(object):\n",
        "    def __init__(self, distill=False, attention=False, student_id='resnet18', teacher_id='resnet34'):\n",
        "        self.distill = distill\n",
        "        self.attention = attention\n",
        "        self.teacher_id = teacher_id\n",
        "        self.teacher = Teacher(pretrained=True, teacher_id=teacher_id) if distill else None\n",
        "        self.model_id = student_id\n",
        "        self.model = None\n",
        "        self.epochs = 200\n",
        "        self.train_batch_size = 128\n",
        "        self.test_batch_size = 128\n",
        "        self.train_loader = None\n",
        "        self.test_loader = None\n",
        "        self.criterion = None\n",
        "        self.optimizer = None\n",
        "        self.scheduler = None\n",
        "\n",
        "    def load_data(self):\n",
        "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.ToTensor()])\n",
        "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
        "        train_set = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=self.train_batch_size,\n",
        "                                                        shuffle=True)\n",
        "        test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=self.test_batch_size, shuffle=False)\n",
        "\n",
        "    def load_model(self):\n",
        "        if self.distill:\n",
        "            self.teacher.run()\n",
        "            for para in list(self.teacher.model.parameters()):\n",
        "                para.requires_grad = False\n",
        "\n",
        "        if self.model_id == 'vgg11':\n",
        "            self.model = VGG(11).to(device)\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "        elif self.model_id == 'resnet18':\n",
        "            self.model = ResNet(18).to(device)\n",
        "            self.optimizer = optim.SGD(self.model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "        # self.model = torch.nn.DataParallel(self.model)\n",
        "        self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, milestones=[75, 150], gamma=0.5)\n",
        "        self.criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train()\n",
        "        train_loss = 0\n",
        "        train_correct = 0\n",
        "        total = 0\n",
        "        if self.distill:\n",
        "            temperature = 0.1\n",
        "            alpha = 1.0\n",
        "        if self.attention:\n",
        "            beta = 0.1\n",
        "\n",
        "        last_time = time.time()\n",
        "        for batch_num, (data, target) in enumerate(self.train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "\n",
        "            if self.distill:\n",
        "                self.teacher.optimizer.zero_grad()\n",
        "                teacher_output = self.teacher.model(data)\n",
        "                kl_loss = F.kl_div(F.log_softmax(output / temperature),\n",
        "                                   F.softmax(teacher_output / temperature),\n",
        "                                   reduction='batchmean')\n",
        "                ce_loss = F.cross_entropy(output, target)\n",
        "                loss += alpha * (temperature ** 2) * kl_loss + (1 - alpha) * ce_loss\n",
        "            if self.attention:\n",
        "                at_loss = 0\n",
        "                for s_gram, t_gram in zip(self.model.grams, self.teacher.model.grams):\n",
        "                    s_at = F.normalize(s_gram.pow(2).mean(1).view(s_gram.size(0), -1))\n",
        "                    t_at = F.normalize(t_gram.pow(2).mean(1).view(t_gram.size(0), -1))\n",
        "                    at_loss += (s_at - t_at).pow(2).mean().sum()\n",
        "                loss += beta * at_loss\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            prediction = torch.max(output, 1)  # second param '1' represents the dimension to be reduced\n",
        "            total += target.size(0)\n",
        "            # train_correct incremented by one if predicted right\n",
        "            train_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
        "\n",
        "        acc_msg = train_correct / total\n",
        "        loss_msg = train_loss / self.train_batch_size\n",
        "        time_msg = time.time() - last_time\n",
        "        return acc_msg, loss_msg, time_msg\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        test_loss = 0\n",
        "        test_correct = 0\n",
        "        total = 0\n",
        "\n",
        "        last_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            for _, (data, target) in enumerate(self.test_loader):\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = self.model(data)\n",
        "                loss = self.criterion(output, target)\n",
        "                test_loss += loss.item()\n",
        "                prediction = torch.max(output, 1)\n",
        "                total += target.size(0)\n",
        "                test_correct += np.sum(prediction[1].cpu().numpy() == target.cpu().numpy())\n",
        "\n",
        "        acc_msg = test_correct / total\n",
        "        loss_msg = test_loss / self.test_batch_size\n",
        "        time_msg = time.time() - last_time\n",
        "        return acc_msg, loss_msg, time_msg\n",
        "\n",
        "    def run(self):\n",
        "        self.load_data()\n",
        "        self.load_model()\n",
        "        best_acc = 0\n",
        "        best_model = None\n",
        "        best_settings = None\n",
        "        init_time = time.time()\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            self.scheduler.step(epoch)\n",
        "            print('==> Epoch %d/200' % (epoch + 1))\n",
        "\n",
        "            train_acc, train_loss, train_time = self.train()\n",
        "            acc_msg = 'TrainAcc %.3f%%' % (train_acc * 100.0)\n",
        "            loss_msg = 'TrainLoss %.3f' % train_loss\n",
        "            time_msg = datetime.fromtimestamp(train_time).strftime('%H:%M:%S')\n",
        "            print('Train:' + ' | '.join([acc_msg, loss_msg, time_msg]))\n",
        "\n",
        "            test_acc, test_loss, test_time = self.test()\n",
        "            acc_msg = 'TestAcc %.3f%%' % (test_acc * 100.0)\n",
        "            loss_msg = 'TestLoss %.3f' % test_loss\n",
        "            time_msg = datetime.fromtimestamp(test_time).strftime('%H:%M:%S')\n",
        "            print('Test:' + ' | '.join([acc_msg, loss_msg, time_msg]))\n",
        "\n",
        "            if best_acc < test_acc:\n",
        "                best_acc = test_acc\n",
        "                best_model = self.model\n",
        "                best_settings = copy.deepcopy(self.model.state_dict())\n",
        "        total_time = time.time() - init_time\n",
        "        total_time_msg = datetime.fromtimestamp(total_time).strftime('%H:%M:%S')\n",
        "        print('==> Best TestAcc %.3f%%' % (best_acc * 100))\n",
        "        print('==> Total Time ' + total_time_msg)\n",
        "        best_model.load_state_dict(best_settings)\n",
        "\n",
        "        Visualization.feature_map({\n",
        "            self.model_id: best_model,\n",
        "            self.teacher_id: self.teacher.model,\n",
        "        })\n",
        "\n",
        "\n",
        "class ResNet34AT(resnet.ResNet):\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        g0 = self.layer1(x)\n",
        "        g1 = self.layer2(g0)\n",
        "        g2 = self.layer3(g1)\n",
        "        g3 = self.layer4(g2)\n",
        "\n",
        "        return [g.pow(2).mean(1) for g in (g0, g1, g2, g3)]\n",
        "\n",
        "\n",
        "class Visualization:\n",
        "    @staticmethod\n",
        "    def _resnet34():\n",
        "        model = ResNet34AT(resnet.BasicBlock, [3, 4, 6, 3])\n",
        "        base_model = resnet34(pretrained=True)\n",
        "        model.load_state_dict(base_model.state_dict())\n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def feature_map(models=None):\n",
        "        urls = {\n",
        "            'bird': 'https://cdn.pixabay.com/photo/2019/05/10/19/48/bird-4194340_960_720.jpg',\n",
        "            'stork': 'https://cdn.pixabay.com/photo/2019/05/08/01/05/stork-4187520_960_720.jpg',\n",
        "        }\n",
        "        is_demo = False\n",
        "        if models is None:\n",
        "            models = {\n",
        "                'resnet34': Visualization._resnet34(),\n",
        "            }\n",
        "            is_demo = True\n",
        "        # downloads = list()\n",
        "        for pic_id, pic_url in urls.items():\n",
        "            response = requests.get(pic_url)\n",
        "            im = np.ascontiguousarray(Image.open(BytesIO(response.content)), dtype=np.uint8)\n",
        "\n",
        "            title = f'{pic_id}'\n",
        "            plt.imshow(im)\n",
        "            plt.title(title)\n",
        "            plt.savefig(f'{title}.jpg', dpi=600)\n",
        "            plt.show()\n",
        "            # downloads.append(f'{title}.jpg')\n",
        "\n",
        "            tr_center_crop = T.Compose([\n",
        "                T.ToPILImage(),\n",
        "                T.Resize(256),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            for model_id, model in models.items():\n",
        "                model.eval()\n",
        "                model.to(torch.device('cpu'))\n",
        "                with torch.no_grad():\n",
        "                    if is_demo:\n",
        "                        x = tr_center_crop(im).unsqueeze(0)\n",
        "                        gs = model(x)\n",
        "                    else:\n",
        "                        x = tr_center_crop(im).unsqueeze(0)\n",
        "                        _ = model(x)\n",
        "                        gs = [g.pow(2).mean(1) for g in model.grams]\n",
        "\n",
        "                for i, g in enumerate(gs):\n",
        "                    title = f'{model_id}-{pic_id}-g{i}'\n",
        "                    plt.imshow(g[0], interpolation='bicubic')\n",
        "                    plt.title(title)\n",
        "                    plt.savefig(f'{title}.jpg', dpi=600)\n",
        "                    plt.show()\n",
        "                    # downloads.append(f'{title}.jpg')\n",
        "        # for download in downloads:\n",
        "        #     files.download(download)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # basic teachers\n",
        "    Teacher().run()\n",
        "    # Teacher(teacher_id='vgg19').run()\n",
        "    # basic students\n",
        "    # Student().run()\n",
        "    # Student(student_id='vgg11').run()\n",
        "    # distillation\n",
        "    # Student(distill=True).run()\n",
        "    # Student(distill=True, student_id='vgg11', teacher_id='vgg19').run()\n",
        "    # distillation with attention\n",
        "    # Student(distill=True, attention=True).run()\n",
        "    # Student(distill=True, attention=True, student_id='vgg11', teacher_id='vgg19').run()\n",
        "    # Demo Attention Maps Visualization\n",
        "    # Visualization.feature_map()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}